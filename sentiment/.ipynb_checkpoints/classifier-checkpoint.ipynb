{
 "metadata": {
  "name": "",
  "signature": "sha256:97c037dada2936d697b5be5efed22d6e55c5686cacf2a13d67df559970d4dec0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# python imports\n",
      "import os, string, gzip\n",
      "from random import shuffle\n",
      "from __future__ import division\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "import numpy as np\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "from sklearn.linear_model import SGDClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# defined function(s)\n",
      "def gzipGenerator(gz_filename):\n",
      "    '''function takes a gzip file name and creates an iterator for that file.'''\n",
      "    with gzip.open(gz_filename, 'rb') as f:\n",
      "        for line in f:\n",
      "            yield line\n",
      "            \n",
      "def extractData(filename, count):\n",
      "    '''function takes a gziped file and a quantity; opens the gzip file and gets the \n",
      "    quantity of of lines specified from the file; returns the list of file lines'''\n",
      "    strings = []\n",
      "    with gzip.open(filename, 'rb') as f:\n",
      "        for line in f:\n",
      "            strings.append(line)\n",
      "            count -= 1\n",
      "            if count == 0: break\n",
      "    return strings\n",
      "\n",
      "def cleanTweet(string_):\n",
      "    '''function takes a string and returns the string modified. The string is converted to lower case,\n",
      "    puncutation & stop words and all words are take to their root form.'''\n",
      "    lowercase = string_.strip().lower()\n",
      "    noPunctuation = lowercase.translate(None, string.punctuation)\n",
      "    noStopWords = [word for word in noPunctuation.split(' ') if word not in stopwords.words('english')]\n",
      "    wnl = WordNetLemmatizer()\n",
      "    rootWords = [wnl.lemmatize(word) for word in noStopWords]\n",
      "    noUrls = [word for word in rootWords if not 'http' in word]\n",
      "    if noUrls and noUrls[0] == 'RT': del noUrls[0]\n",
      "    return ' '.join(noUrls) + '\\n'\n",
      "\n",
      "def twitterCleaner(directory, quantity, outputfile):\n",
      "    '''function takes a directory path and a output file name, gets the list of gzip file names from\n",
      "    the specified directory, then iterates through each file iterating through each line of the specified\n",
      "    twitter csv file and outputs a clean version of the tweet to the gziped output file.'''\n",
      "    gzipfiles = [f for f in os.listdir(directory)]\n",
      "    with gzip.open('twitter-tweets/' + outputfile + '.txt.gz', 'wb') as f:\n",
      "        for gzipfile in sorted(gzipfiles):\n",
      "            for tweet in gzipGenerator(directory + gzipfile):\n",
      "                f.write(cleanTweet(tweet.split('|;|')[-1]))\n",
      "                quantity -= 1\n",
      "                if quantity == 0: break\n",
      "            if quantity == 0: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number of reviews to train on\n",
      "numElements = 100000\n",
      "\n",
      "# get the negative Amazom review data and create a list of tuples (review, neg-lable)\n",
      "x_neg = np.array(extractData('amazon-reviews/onestar-clean.txt.gz', numElements))\n",
      "y_neg = [0] * numElements\n",
      "train_neg = zip(x_neg, y_neg)\n",
      "\n",
      "# get the positive Amazom rewiew data and create a list of tuples (review, pos-lable)\n",
      "x_pos = np.array(extractData('amazon-reviews/fivestar-clean.txt.gz', numElements))\n",
      "y_pos = [1] * numElements\n",
      "train_pos = zip(x_pos, y_pos)\n",
      "\n",
      "# combine and shuffle the positive / negative data\n",
      "data = train_neg + train_pos\n",
      "shuffle(data)\n",
      "\n",
      "# take all the the reviews and break it into data and target \n",
      "dataReviews = zip( * data)\n",
      "x_dataReviews, y_dataReviews = dataReviews[0], dataReviews[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the pre rated tweets\n",
      "with open('ratedTweetsCorrected.txt', 'r') as f:\n",
      "    ratedTweets = f.readlines()\n",
      "    \n",
      "# clean the tweets and put into arrays\n",
      "x_tweets = []\n",
      "y_tweets = []\n",
      "for ratedTweet in ratedTweets:\n",
      "    y, x = ratedTweet.strip().split('|;|')\n",
      "    x_tweets.append(cleanTweet(x).strip()), y_tweets.append(int(y))\n",
      "    \n",
      "# convert to numpy arrays\n",
      "x_tweets = np.array(x_tweets)\n",
      "y_tweets = np.array(y_tweets)\n",
      "\n",
      "# convert all -1 to 0\n",
      "y_tweets[y_tweets == -1] = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create our pipeline with our transformers and classifier\n",
      "pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(n_jobs=-1))])\n",
      "\n",
      "# set parameters bases on GridSearch results\n",
      "pipeline.set_params(clf__alpha=1e-05, clf__loss='hinge', clf__n_iter=80, clf__penalty='l2',\n",
      "                    tfidf__norm='l2', tfidf__smooth_idf=True, tfidf__use_idf=True,\n",
      "                    vect__max_df=0.5, vect__max_features=50000, vect__ngram_range=(1, 3))\n",
      "\n",
      "# fit the training data\n",
      "pipeline.fit(x_dataReviews, y_dataReviews)\n",
      "\n",
      "# get predictions\n",
      "predictions = pipeline.predict(x_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get score\n",
      "print 'prediction score: %0.3f%c' % ((pipeline.score(x_tweets, y_tweets) * 100), '%')\n",
      "\n",
      "# validate that clasifier is not predicting all as neg or pos\n",
      "tuples = zip(y_tweets, predictions)\n",
      "neg = pos = neg_wrong = pos_wrong = 0\n",
      "for tuple_ in tuples:\n",
      "    if tuple_[0] == 0: neg += 1\n",
      "    if tuple_[0] == 0 and tuple_[1] == 1: neg_wrong += 1\n",
      "    if tuple_[0] == 1: pos += 1\n",
      "    if tuple_[0] == 1 and tuple_[1] == 0: pos_wrong += 1\n",
      "print 'percent of incorrect predictions that are negative: %0.3f%c and positive: %0.3f%c' % (((neg_wrong / neg) * 100), '%', ((pos_wrong / pos) * 100), '%')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "prediction score: 73.714%\n",
        "percent of incorrect predictions that are negative: 29.961% and positive: 22.761%\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}