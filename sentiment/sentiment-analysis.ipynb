{
 "metadata": {
  "name": "",
  "signature": "sha256:2494622d67a380e5e8821df13923924d247791c3474c3ae18c7ad112bdaca922"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preparation Phase"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Acquire Data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Amazon reviews were downloaded from the Stanford Large Network Dataset Collection: http://snap.stanford.edu/data/\n",
      "Twitter tweets were collected via Twitter's API"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reformat and Clean Data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We need to clean the data before we can use it. The Amazon reviews will need to have all punctuation and stop words removed; the words that remain will be converted to lower case and to their root word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# python imports\n",
      "import string, gzip, nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# defined function(s)\n",
      "def cleanString(string_):\n",
      "    '''function takes a string and returns the string modified. The string is converted to lower case,\n",
      "    puncutation & stop words are removed, and all words are take to their root form.'''\n",
      "    lowercase = string_.strip().lower()\n",
      "    noPunctuation = lowercase.translate(None, string.punctuation)\n",
      "    noStopWords = [word for word in noPunctuation.split(' ') if word not in stopwords.words('english')]\n",
      "    wnl = WordNetLemmatizer()\n",
      "    rootWords = [wnl.lemmatize(word) for word in noStopWords]\n",
      "    return ' '.join(rootWords) + '\\n'\n",
      "\n",
      "def gzipGenerator(gz_filename):\n",
      "    '''function takes a gzip file name and creates an iterator for that file.'''\n",
      "    with gzip.open(gz_filename, 'rb') as f:\n",
      "        for line in f:\n",
      "            yield line\n",
      "            \n",
      "def fileCleaner(gz_filename, outputfile):\n",
      "    '''function takes a gzip file name and a output file name and iterates through each line of \n",
      "    the specified textfile and outputs a clean version of the review to the gziped output file.'''\n",
      "    with gzip.open(outputfile + '.txt.gz', 'wb') as f:\n",
      "        for review in gzipGenerator(gz_filename):\n",
      "            f.write(cleanString(review))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clean the one and five star Amazon reviews, output to a gziped file\n",
      "fileCleaner('amazon-reviews/very-neg.txt.gz', 'amazon-reviews/onestar-clean')\n",
      "fileCleaner('amazon-reviews/very-pos.txt.gz', 'amazon-reviews/fivestar-clean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysis Phase"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classification and Perdiction"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "For this part we will extract n number of Amazon reviews from both the positive and negative files. We'll create our training set by combining the positive and negative reviews into one array, then shuffle the data. To create a test set, we need to save a portion of out training data, for this example we'll save 10% of the data for testing. \n",
      "\n",
      "Now that we have our training and testing sets we can create our classifier. We'll use a Pipeline of transformations with a final estimator in order to assemble the steps, while trying different parameters. In our case the transformations will be a CountVectorizer and a TfidfTransformer. The CountVector converts a collection of texts into a matrix of token counts and the TfidfTransformer transforms token counts to a normalized tf\u2013idf (term-frequency times inverse document-frequency); in other words, it scales down the impact of tokens that occur very frequently in a given corpus. In order to decide what parameters we need to choose and to cross validate or results we'll use GridSearchCV. \n",
      "\n",
      "References:\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\n",
      "http://scikit-learn.org/stable/auto_examples/grid_search_digits.html\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# python imports\n",
      "import numpy as np\n",
      "from random import shuffle\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "from sklearn.linear_model import SGDClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# defined function(s)\n",
      "def extractData(filename, count):\n",
      "    '''function takes a gziped file and a quantity; opens the gzip file and gets the \n",
      "    quantity of of lines specified from the file; returns the list of file lines'''\n",
      "    strings = []\n",
      "    with gzip.open(filename, 'rb') as f:\n",
      "        for line in f:\n",
      "            strings.append(line)\n",
      "            count -= 1\n",
      "            if count == 0: break\n",
      "    return strings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number of reviews and percentage to test on\n",
      "numElements = 1000\n",
      "percentage = 0.1\n",
      "\n",
      "# get the negative Amazom review data and create a list of tuples (review, neg-lable)\n",
      "x_neg = np.array(extractData('amazon-reviews/onestar-clean.txt.gz', numElements))\n",
      "y_neg = [0] * numElements\n",
      "train_neg = zip(x_neg, y_neg)\n",
      "\n",
      "# get the positive Amazom rewiew data and create a list of tuples (review, pos-lable)\n",
      "x_pos = np.array(extractData('amazon-reviews/fivestar-clean.txt.gz', numElements))\n",
      "y_pos = [1] * numElements\n",
      "train_pos = zip(x_pos, y_pos)\n",
      "\n",
      "# combine and shuffle the positive / negative data\n",
      "data = train_neg + train_pos\n",
      "shuffle(data)\n",
      "\n",
      "# create breat point of train / test sets\n",
      "breakPoint = int(len(data) * percentage)\n",
      "\n",
      "# seperate the tuple (x_review, y_lable) into the train / test sets\n",
      "train = zip( * data[breakPoint:])\n",
      "x_train, y_train = train[0], train[1]\n",
      "test = zip( * data[:breakPoint])\n",
      "x_test, y_test = test[0], test[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create our pipeline with our transformers and classifier\n",
      "pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(n_jobs=-1))])\n",
      "\n",
      "# define our parameter that we want to test on our transformers and classifer during our grid search\n",
      "parameters = {'vect__max_df': (0.25, 0.5, 0.75, 1.0), # ignore terms that have a term frequency higher than threshold\n",
      "              'vect__max_features': (None, 5000, 10000, 50000), # builds a vocabulary with only the top max_features\n",
      "              'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), # lower/upper boundary of range of n-values for different n-grams to be extracted\n",
      "              'tfidf__smooth_idf': (True, False), # smooth idf weights by adding one to document frequencies, prevents zero division\n",
      "              'tfidf__use_idf': (True, False), # inverse-document-frequency reweighting\n",
      "              'tfidf__norm': ('l1', 'l2'), # used to normalize term vectors\n",
      "              'clf__loss': ('hinge', #  linear support vector machine (SVM)\n",
      "                            #'log', # logistic regression\n",
      "                            #'modified_huber', # smooth loss that brings tolerance to outliers as well as probability estimates\n",
      "                            #'squared_hinge', #  is like hinge but is quadratically penalized\n",
      "                            #'perceptron', # is the linear loss used by the perceptron algorithm\n",
      "                            #'squared_loss', # ordinary least squares fit\n",
      "                            #'huber', # modifies \u2018squared_loss\u2019 to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon\n",
      "                            #'epsilon_insensitive', # ignores errors less than epsilon and is linear past that; this is the loss function used in SVR\n",
      "                            #'squared_epsilon_insensitive' # same as epsilon_insensitive, but becomes squared loss past a tolerance of epsilon\n",
      "                            ),\n",
      "              'clf__alpha': (0.00001, 0.000001, 0.0000001), # constant that multiplies the regularization term\n",
      "              'clf__penalty': ('l1', 'l2', 'elasticnet'), # penalty (aka regularization term) to be used\n",
      "              'clf__n_iter': (10, 20, 40, 80)} # number of passes over the training data\n",
      "\n",
      "# create our grid search object and fit it to our data\n",
      "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, pre_dispatch=4, cv=5, verbose=1)\n",
      "grid_search.fit(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.5s\n",
        "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:  1.5min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:  6.5min\n",
        "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed: 14.1min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed: 25.3min\n",
        "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed: 39.2min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed: 56.6min\n",
        "[Parallel(n_jobs=-1)]: Done 2450 jobs       | elapsed: 76.6min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 3200 jobs       | elapsed: 100.0min\n",
        "[Parallel(n_jobs=-1)]: Done 4050 jobs       | elapsed: 144.0min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 5000 jobs       | elapsed: 174.4min\n",
        "[Parallel(n_jobs=-1)]: Done 6050 jobs       | elapsed: 207.2min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 7200 jobs       | elapsed: 243.6min\n",
        "[Parallel(n_jobs=-1)]: Done 8450 jobs       | elapsed: 283.1min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 9800 jobs       | elapsed: 326.3min\n",
        "[Parallel(n_jobs=-1)]: Done 11250 jobs       | elapsed: 371.8min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 12800 jobs       | elapsed: 420.6min\n",
        "[Parallel(n_jobs=-1)]: Done 14450 jobs       | elapsed: 473.3min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 16200 jobs       | elapsed: 530.2min\n",
        "[Parallel(n_jobs=-1)]: Done 18050 jobs       | elapsed: 590.9min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 20000 jobs       | elapsed: 655.6min\n",
        "[Parallel(n_jobs=-1)]: Done 22050 jobs       | elapsed: 725.1min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 24200 jobs       | elapsed: 801.8min\n",
        "[Parallel(n_jobs=-1)]: Done 26450 jobs       | elapsed: 879.7min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 28800 jobs       | elapsed: 961.6min\n",
        "[Parallel(n_jobs=-1)]: Done 31250 jobs       | elapsed: 1044.3min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 33800 jobs       | elapsed: 1124.6min\n",
        "[Parallel(n_jobs=-1)]: Done 36450 jobs       | elapsed: 1207.5min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 39200 jobs       | elapsed: 1294.9min\n",
        "[Parallel(n_jobs=-1)]: Done 42050 jobs       | elapsed: 1386.1min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 45000 jobs       | elapsed: 1481.1min\n",
        "[Parallel(n_jobs=-1)]: Done 48050 jobs       | elapsed: 1580.3min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 51200 jobs       | elapsed: 1681.0min\n",
        "[Parallel(n_jobs=-1)]: Done 54450 jobs       | elapsed: 1789.5min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 57800 jobs       | elapsed: 1901.6min\n",
        "[Parallel(n_jobs=-1)]: Done 61250 jobs       | elapsed: 2018.4min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 64800 jobs       | elapsed: 2182.0min\n",
        "[Parallel(n_jobs=-1)]: Done 68450 jobs       | elapsed: 2344.4min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 72200 jobs       | elapsed: 2465.2min\n",
        "[Parallel(n_jobs=-1)]: Done 76050 jobs       | elapsed: 2664.6min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 80000 jobs       | elapsed: 2792.5min\n",
        "[Parallel(n_jobs=-1)]: Done 84050 jobs       | elapsed: 2926.8min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done 88200 jobs       | elapsed: 3066.1min\n",
        "[Parallel(n_jobs=-1)]: Done 92160 out of 92160 | elapsed: 3200.2min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 5 folds for each of 18432 candidates, totalling 92160 fits\n",
        "Best Score: 0.893"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "not enough arguments for format string",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-18-9f7f0066b06f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Best Parameters:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'\\tname, value: %s, %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: not enough arguments for format string"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Parameters:\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# display our results\n",
      "print 'Best Score:\\n\\t~ %0.3f' % grid_search.best_score_\n",
      "best_parameters = grid_search.best_estimator_.get_params()\n",
      "print 'Best Parameters:'\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print '\\t~', param_name, best_parameters[param_name]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best Score:\n",
        "\t~ 0.893\n",
        "Best Parameters:\n",
        "\t~ clf__alpha 1e-05\n",
        "\t~ clf__loss hinge\n",
        "\t~ clf__n_iter 80\n",
        "\t~ clf__penalty l2\n",
        "\t~ tfidf__norm l2\n",
        "\t~ tfidf__smooth_idf True\n",
        "\t~ tfidf__use_idf True\n",
        "\t~ vect__max_df 0.5\n",
        "\t~ vect__max_features 50000\n",
        "\t~ vect__ngram_range (1, 3)\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Now that we've established what parameters are working best for both our transformers and classifiers, using GridSearch, we will build a new Pipeline. We will use this Pipeline to lable and score our test set. \n",
      "\n",
      "Usualy we would need to perform some sort of cross validation, in order to validate our results below; however, we aready preformed a cross validation using five folds on the data, during the GridSearch process."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create our pipeline with our transformers and classifier\n",
      "pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(n_jobs=-1))])\n",
      "\n",
      "# set parameters bases on GridSearch results\n",
      "pipeline.set_params(clf__alpha=1e-05, clf__loss='hinge', clf__n_iter=80, clf__penalty='l2',\n",
      "                    tfidf__norm='l2', tfidf__smooth_idf=True, tfidf__use_idf=True,\n",
      "                    vect__max_df=0.5, vect__max_features=50000, vect__ngram_range=(1, 3))\n",
      "\n",
      "# fit the training data\n",
      "pipeline.fit(x_train, y_train)\n",
      "\n",
      "# get score of test set\n",
      "print 'Score:', pipeline.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.89\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Reflection"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Make Comparisons and Explore Alternatives"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We'll take n number of Twitter tweets and use our classifier above to perdict their lables, negative or positive. The tweets and their lables will be displayed for us to review and make comparisons as to whether or not our classifier is working corectly, or at least the way we hpoed for. If we are not receiving the results we had hoped for, we'll need to look at alternatives to modify our strategy.\n",
      "\n",
      "A couple of things to take note of:\n",
      "1. after reviewing the results of our perdictor, spelling errors in the tweets seems to be our greatest enemy\n",
      "2. we need to look into Amazons High Performance Computing (HPC), or something like it, as not only did it take over two days to produce the classifier we have now, but we had many issues with floating point errors and overflow while searching for our best parameters. In the end it took us over five days to create our classifier and look at our predictions to make comparasions. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# python imports\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# defined function(s)\n",
      "def cleanTweet(string_):\n",
      "    '''function takes a string and returns the string modified. The string is converted to lower case,\n",
      "    puncutation & stop words and all words are take to their root form.'''\n",
      "    lowercase = string_.strip().lower()\n",
      "    noPunctuation = lowercase.translate(None, string.punctuation)\n",
      "    noStopWords = [word for word in noPunctuation.split(' ') if word not in stopwords.words('english')]\n",
      "    wnl = WordNetLemmatizer()\n",
      "    rootWords = [wnl.lemmatize(word) for word in noStopWords]\n",
      "    noUrls = [word for word in rootWords if not 'http' in word]\n",
      "    if noUrls and noUrls[0] == 'RT': del noUrls[0]\n",
      "    return ' '.join(noUrls) + '\\n'\n",
      "\n",
      "def twitterCleaner(directory, quantity, outputfile):\n",
      "    '''function takes a directory path and a output file name, gets the list of gzip file names from\n",
      "    the specified directory, then iterates through each file iterating through each line of the specified\n",
      "    twitter csv file and outputs a clean version of the tweet to the gziped output file.'''\n",
      "    gzipfiles = [f for f in os.listdir(directory)]\n",
      "    with gzip.open('twitter-tweets/' + outputfile + '.txt.gz', 'wb') as f:\n",
      "        for gzipfile in sorted(gzipfiles):\n",
      "            for tweet in gzipGenerator(directory + gzipfile):\n",
      "                f.write(cleanTweet(tweet.split('|;|')[-1]))\n",
      "                quantity -= 1\n",
      "                if quantity == 0: break\n",
      "            if quantity == 0: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clean the Twitter tweets and output to a gziped file\n",
      "twitterCleaner('twitter-tweets/', 2000, 'tweets-clean')\n",
      "\n",
      "# get n tweets to be perdicted\n",
      "x_tweets = np.array(extractData('twitter-tweets/tweets-clean.txt.gz', 100))\n",
      "\n",
      "# take all the the reviews and break it into data and target \n",
      "dataReviews = zip( * data)\n",
      "x_dataReviews, y_dataReviews = dataReviews[0], dataReviews[1]\n",
      "\n",
      "# fit the training data\n",
      "pipeline.fit(x_dataReviews, y_dataReviews)\n",
      "    \n",
      "# get perdiction and score of test set\n",
      "y_tweets = pipeline.predict(x_tweets)\n",
      "\n",
      "# zip tweets and lables together and print\n",
      "labledTweets = zip(y_tweets, x_tweets)\n",
      "for labledTweet in labledTweets:\n",
      "    print 'Tweet[' + str(labledTweet[0]) + ']: ' + labledTweet[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tweet[0]: rt xstrology libra get irritated dont feel appreciated nothing upset\n",
        "\n",
        "Tweet[0]: rt soundersfc wanr win two free ticket sfcluncheon here chance\n",
        "\n",
        "Tweet[0]: hossdelgado iight shid frfr hml 636 7024\n",
        "\n",
        "Tweet[1]: high school boy basketball top 25 composite ranking\n",
        "\n",
        "Tweet[0]: rubbermaid 5e28 deluxe tool tower rack caster hold 40 tool find biggest selection product fr\n",
        "\n",
        "Tweet[0]: benmckenna28 haha harsh ben \n",
        "\n",
        "Tweet[0]: rt cnet democratic senator demand complete u ban bitcoin\n",
        "\n",
        "Tweet[1]: rt ilikesexdaily pull hair\n",
        "\n",
        "Tweet[1]: rt emahleh kinda want puke kinda wanna cry kinda wanna punch someone face\n",
        "\n",
        "Tweet[0]: tatilaughs yes blanco acting rn\n",
        "\n",
        "Tweet[0]: rt ohwinsagain fucking smug eh fired prison good\n",
        "\n",
        "Tweet[0]: ellemno elly\n",
        "\n",
        "Tweet[1]: heath class watched squirrel video best thing ever\n",
        "\n",
        "Tweet[1]: glad managed make helicopter extinction even though couldnt communicate team mate\n",
        "\n",
        "Tweet[0]: missfavitta hand finger toe raised\n",
        "\n",
        "Tweet[0]: rt johnmoore7 back goin\n",
        "\n",
        "Tweet[1]: hugsieee hope check get chance \n",
        "\n",
        "Tweet[1]: finished painting disney bowl peer help line pottery  event minnieme\n",
        "\n",
        "Tweet[0]: rt pisceshc pisces doesnt care 1am 2am 3am 4am call care answer\n",
        "\n",
        "Tweet[1]: rt thegooglefactz quokka happiest animal specie world\n",
        "\n",
        "Tweet[0]: 360 million newly stolen credential sale cnbc\n",
        "\n",
        "Tweet[0]: \n",
        "\n",
        "Tweet[0]: working finishing cover pyromania issue 2 amusing greatly\n",
        "\n",
        "Tweet[0]: seems everyone encouraging thing p cancer\n",
        "\n",
        "Tweet[0]: rt realmadrid 25 schalke 02 real madrid rmlive\n",
        "\n",
        "Tweet[0]: rt vejaytarianfr someone said max georgeous  kca votethewanteduk\n",
        "\n",
        "Tweet[0]: waited till 3 oclock today one class lasted 10 minute\n",
        "\n",
        "Tweet[0]: rt shmoplifelouie reckless bitch cant stand\n",
        "\n",
        "Tweet[0]: good know care much\n",
        "\n",
        "Tweet[1]: rt militaryporn cockpit view f35\n",
        "\n",
        "Tweet[0]: nxvnaade homacides drxnks dxubt isthatnicksmith gxntain sorry thats best could guess\n",
        "\n",
        "Tweet[0]: nomorejacketsdemiyourarmsareperfect sitevasdecarnavalpierdesavzla\n",
        "\n",
        "Tweet[1]: real man would leave girl behind gtgtgtgtgt\n",
        "\n",
        "Tweet[1]: somebody agrees  gtgtgtgt 2014 year greece find growth\n",
        "\n",
        "Tweet[0]: bored stay home lazy go\n",
        "\n",
        "Tweet[1]: im really craving big really good burrito ugh hate stuck work want food\n",
        "\n",
        "Tweet[1]: round 2 onnnn\n",
        "\n",
        "Tweet[0]: rt miieecirus rt yuo cri evrytiem\n",
        "\n",
        "Tweet[0]: annoyed af\n",
        "\n",
        "Tweet[1]: rt greaterthn graduation speech\n",
        "\n",
        "Tweet[1]: youll never ever care\n",
        "\n",
        "Tweet[0]: ae even feel like fkinn shouldnyyyy\n",
        "\n",
        "Tweet[0]: knee kind ofmessed\n",
        "\n",
        "Tweet[1]: one group people gentrification lifestyle choice another there choice money power bestows\n",
        "\n",
        "Tweet[1]: feel like never ending sickness\n",
        "\n",
        "Tweet[0]: rt justinbieber sad medium doesnt believe fact checking anymore report real thing fake thing wow \n",
        "\n",
        "Tweet[1]: people lie bed listening music\n",
        "\n",
        "Tweet[0]: rt niallofficial please support  important\n",
        "\n",
        "Tweet[0]: yes im talking\n",
        "\n",
        "Tweet[0]: nashgrier glad dont live near buffalo\n",
        "\n",
        "Tweet[1]: prove traditional practice  scripture reference sound scriptural\n",
        "\n",
        "Tweet[0]: oh fuck sake crowley\n",
        "\n",
        "Tweet[1]: like there weird variety music\n",
        "\n",
        "Tweet[0]: rt realmadrid 25 schalke 02 real madrid rmlive\n",
        "\n",
        "Tweet[1]: rt soreiatable need two hour long hug\n",
        "\n",
        "Tweet[0]: drinknseek thing worse conversation youre sitting awkward\n",
        "\n",
        "Tweet[1]: rt erikwlundstrom want another 4 day weekend\n",
        "\n",
        "Tweet[1]: whats good im taking zoukfridays feb28th amp wanted come party wme missnatalienunn amp meghanjames bgc ayeelilbit\n",
        "\n",
        "Tweet[0]: rt totaltrafficroc brighton accident e henrietta rd nbsb jefferson rd reported dot traffic\n",
        "\n",
        "Tweet[0]: home tired\n",
        "\n",
        "Tweet[1]: thank jdbavonking followback\n",
        "\n",
        "Tweet[0]: rt stlouisrams happy birthday super bowl xxxiv champion profootballhof running back marshallfaulk\n",
        "\n",
        "Tweet[1]: 3 wish\n",
        "\n",
        "Tweet[1]: keeping stepping\n",
        "\n",
        "Tweet[0]: cristinalake didnt kiss formidable as\n",
        "\n",
        "Tweet[1]: last time discovered something new 28 artist 13 genre 1 free album  next 2 hr\n",
        "\n",
        "Tweet[0]: rt mandimoonlite bet u \n",
        "\n",
        "Tweet[1]: scousemouse1982 wheyheyofficial go whey lmaoooooo\n",
        "\n",
        "Tweet[0]: jydex91 workinganoda one plsss\n",
        "\n",
        "Tweet[1]: rt alexparker05 retweet dm justinbieber lt3 must following  parkerfamily\n",
        "\n",
        "Tweet[1]: rt hackneycouncil mayor pipe say central government giving one hand taking away fullcouncil\n",
        "\n",
        "Tweet[0]: hi im nana mention u want talk im online ill back \n",
        "\n",
        "Tweet[0]: game developer arent disappearing toward aspire road give rise game sel\n",
        "\n",
        "Tweet[1]: never relationship anyoneya u used flirty somtimes\n",
        "\n",
        "Tweet[0]: u still expect marry pious woman great \n",
        "\n",
        "Tweet[1]: bringmethebandx seen live\n",
        "\n",
        "Tweet[1]: drpampillay good night sweet dream tk cr\n",
        "\n",
        "Tweet[1]: added video youtube playlist 610 surprise egg unboxing kinder surprise water painting batman car\n",
        "\n",
        "Tweet[0]: im hungry \n",
        "\n",
        "Tweet[0]: ridiculously game creamfields year\n",
        "\n",
        "Tweet[0]: revnickk energy drinkit existstrevagte revnickk gwe wtf pussy energy drink starter main dish\n",
        "\n",
        "Tweet[0]: rt wesleystromberg bros bandlife high five whoever find mom\n",
        "\n",
        "Tweet[1]: jennawolfe  happy birthday jenna  hottest 40 year old ive seen  \n",
        "\n",
        "Tweet[1]: jasenewell ooh guilty crown good im currently loving kill la kill slice life stuff right\n",
        "\n",
        "Tweet[1]: ricenbeats ha ive heard 3 people office say better rsvp fader\n",
        "\n",
        "Tweet[1]: much strength amp power calm amp saying gotta say bc youve done work know amp isnt u melaniefiona\n",
        "\n",
        "Tweet[0]: rt realdopeposts flirt whats mine\n",
        "\n",
        "Tweet[0]: rt banderson30 hey look thatbaseball back\n",
        "\n",
        "Tweet[1]: codysimpson love cody make happy\n",
        "\n",
        "Tweet[0]: saltybalthy queenrinacat fezturions carryonmywaywardsamanddean icejade lol  lucifer lord yolo\n",
        "\n",
        "Tweet[1]: last time discovered something new 28 artist 13 genre 1 free album  next 2 hr\n",
        "\n",
        "Tweet[0]: rt realmadrid 20 gol gol gol gol gol gol gol gol gol gol gol gol gol gol de bale rmlive\n",
        "\n",
        "Tweet[1]: rt estrellagr beautiful dirty rich\n",
        "\n",
        "Tweet[0]: nicest email customer experience approach problem egomassage\n",
        "\n",
        "Tweet[0]: rt markwright partywright itv2 9pm\n",
        "\n",
        "Tweet[0]: rt westpoplar disrespect anybody fuck im gonna check\n",
        "\n",
        "Tweet[0]: devonblanderson iit still hurt bit daddy\n",
        "\n",
        "Tweet[1]: happy wanted wednesday kca votethewanteduk\n",
        "\n",
        "Tweet[0]: rt onedirection listen guy harrystyles special announcement  1dhq x\n",
        "\n",
        "Tweet[1]: like egg fried fertilized yung hunna\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}